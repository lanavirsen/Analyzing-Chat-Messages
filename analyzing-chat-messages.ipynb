{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5537f9-8c03-4592-a52c-e480ce4d4955",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## About this Project\n",
    "\n",
    "This project is based on a dataset derived from a gaming community that operates within a simulated medical emergency service in a science fiction universe. The gameplay involves scenarios where a player character, referred to as the \"client\", experiences incapacitation or isolation. In response, a coordinated team of players, termed the \"responding team\", is mobilized to provide assistance. Communication between the responding team and the client is facilitated through a text-based chat system established for each incident.\n",
    "\n",
    "The objective of this analysis is to identify recurring patterns and commonalities in the chat communications. This exploration assists the community's leadership in determining the feasibility of automating certain repetitive messages, thereby streamlining operations and enhancing response efficiency.\n",
    "\n",
    "Despite the dataset originating from the context of a video game, the analytical techniques and insights gained from this project are universally applicable to a wide range of industries. The core challenge addressed in this analysis - identifying and automating repetitive communication patterns - is prevalent in numerous settings where service representatives interact with clients.\n",
    "\n",
    "The methodology employed here can be adapted to enhance customer service efficiency, reduce response times, and improve client satisfaction in various sectors, including healthcare, customer support, and tech support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c842c-538c-4e27-83d9-5a71b26b6f7f",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "The dataset utilized in this analysis is strictly for internal use due to containing in-game usernames. As a commitment to respecting privacy and upholding ethical standards in data handling, the dataset itself will not be shared publicly.\n",
    "\n",
    "In any outputs or examples provided within this project documentation or related presentations, identifiable information has been anonymized or replaced with placeholders such as `[Organization]`, `[PlayerName]`, `[Location]`, and similar terms. The findings and methodologies are shared openly for educational and demonstrative purposes while ensuring the privacy and anonymity of the individuals involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483fd89-a434-456f-91d2-73de7e8c7a3c",
   "metadata": {},
   "source": [
    "### The Dataset's Description\n",
    "\n",
    "- The dataset has 6 columns and 16&nbsp;509 rows.\n",
    "- Data coverage:\n",
    "  - Start date: June 18, 2023\n",
    "  - End date: March 21, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea6734-121c-43e3-8664-a99cc8ac6a1c",
   "metadata": {},
   "source": [
    "The dataset's columns listed below are the result of normalizing data from `json` format and renaming the columns for clarity. Detailed description for each column:\n",
    "\n",
    "| Column name            | Description                                                 | Type     |\n",
    "|------------------------|-------------------------------------------------------------|----------|\n",
    "| ID                     | Unique identifier for each message within the database      | Text     |\n",
    "| contents               | Text content of the chat message sent between players       | Text     |\n",
    "| created                | Date and time when the message was recorded in the system   | Datetime |\n",
    "| emergency ID           | Identifier linking the message to a specific emergency case | Text     |\n",
    "| sender ID              | Unique identifier of the player who sent the message        | Text     |\n",
    "| message sent timestamp | Date and time when the message was sent by the player       | Datetime |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f6bd3-e6e5-4bfc-a1f7-e69ead8e2037",
   "metadata": {},
   "source": [
    "## The Tools I Use:\n",
    "\n",
    "- **Python:** Programming language for data manipulation and analysis.\n",
    "- **Pandas:** Python library for efficient data cleaning and transformation.\n",
    "- **NLTK (Natural Language Toolkit):** Python toolkit for natural language processing and text analysis.\n",
    "- **Scikit-learn:** Machine learning library for building predictive models.\n",
    "- **Matplotlib:** Visualization tool for creating plots and graphs.\n",
    "- **Jupyter Notebook:** Interactive environment for documenting data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c6784-d1d8-4743-97da-55c0ef7d9a35",
   "metadata": {},
   "source": [
    "# 1. Setting up the Environment\n",
    "\n",
    "The following command installs the Python libraries required for this project:\n",
    "\n",
    "```\n",
    "pip install pandas matplotlib nltk scikit-learn ipython\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea8ea9-ca12-4d9a-84b4-267cde8db4b8",
   "metadata": {},
   "source": [
    "# 2. Loading and Preprocessing the Data\n",
    "## 2.1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecff9f-077b-4e96-a6ce-49fe2747f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "# Specific function from pandas for flattening JSON objects into a flat table.\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Visualization tool for creating plots and graphs.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For parsing JSON data.\n",
    "import json\n",
    "\n",
    "# Toolkit for natural language processing and text analysis.\n",
    "import nltk\n",
    "\n",
    "# List of common words to filter out from text.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to split text into individual words (tokens).\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Transform texts into a suitable format for analysis.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Unsupervised machine learning algorithm for clustering.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Measures how similar an object is to its own cluster compared\n",
    "# to other clusters.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# For topic modeling.\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Provides regular expression matching operations.\n",
    "import re\n",
    "\n",
    "# For interacting with the operating system.\n",
    "import os\n",
    "\n",
    "# For richer output formatting in Jupyter Notebooks.\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Downloading the necessary NLTK datasets.\n",
    "nltk.download('punkt')  # Tokenizer for breaking text into individual words.\n",
    "nltk.download('stopwords')  # Common words to filter out from text.\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # To display all DataFrame rows.\n",
    "pd.set_option('display.max_columns', None)  # To display all DataFrame columns.\n",
    "\n",
    "# Setting max column width to 1000 characters.\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8457da-7a9f-4b73-b379-428e6a8b2086",
   "metadata": {},
   "source": [
    "## 2.2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc5f61-a1cb-48f2-9ffd-3e977f0266b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty list to store each parsed JSON object.\n",
    "data = []\n",
    "\n",
    "# Initializing a string to collect pieces of JSON spread across multiple lines.\n",
    "partial_json = \"\"\n",
    "\n",
    "# Opening and reading the dataset file.\n",
    "with open('chatMessage.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            # Attempting to parse the line with any previously collected\n",
    "            # JSON fragments.\n",
    "            parsed_json = json.loads(partial_json + line)\n",
    "\n",
    "            # If parsing is successful, appending the JSON object to the list\n",
    "            # and resetting partial_json.\n",
    "            data.append(parsed_json)\n",
    "\n",
    "            # Clearing the partial_json to start fresh for the next object.\n",
    "            partial_json = \"\"\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON decoding fails due to incomplete JSON fragments,\n",
    "            # appending the line to partial_json to complete the JSON object.\n",
    "            partial_json += line\n",
    "\n",
    "# Loading the collected JSON objects into a DataFrame.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the first few rows of the DataFrame to ensure data is loaded\n",
    "# properly.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a070e-82aa-4303-acb7-acfb2bb19943",
   "metadata": {},
   "source": [
    "Our JSON object reveals to have a nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836065b8-0b56-46cd-af1a-ee678bc42df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing nested JSON data from the \"Item\" column to create a flat\n",
    "# table structure.\n",
    "df_normalized = json_normalize(df['Item'])\n",
    "\n",
    "# Combining the normalized data back with the original DataFrame,\n",
    "# and dropping the original \"Item\" column which contained nested JSON.\n",
    "chat = pd.concat([df.drop('Item', axis=1), df_normalized], axis=1)\n",
    "\n",
    "# Displaying the first five rows of the processed DataFrame to verify\n",
    "# that it loaded and normalized correctly.\n",
    "chat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ee895-abba-4d2b-bf10-44b33b755006",
   "metadata": {},
   "source": [
    "The JSON normalization process converts nested structures into a flat table by using the path to each element in the nested JSON as the column name. The suffixes in each column name, such as `.S` in `id.S`, `contents.S`, `created.S`, etc., and `.N` in `messageSentTimestamp.N`, indicate the data type or structure from the original JSON object. Here, `.S` stands for a string type, and `.N` in `messageSentTimestamp.N` denotes a numerical type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983089dc-c4fe-4198-881f-5eb7cbdbc8a3",
   "metadata": {},
   "source": [
    "## 2.3. Renaming Columns\n",
    "\n",
    "To improve the readability and usability of our dataset within the analysis, I will remove the `.S` and `.N` suffixes from each column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b05182-c94f-489b-9ea5-ba31bde66df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to more descriptive and simpler names.\n",
    "chat = chat.rename(\n",
    "    columns={\n",
    "        'id.S': 'ID',\n",
    "        'contents.S': 'contents',\n",
    "        'created.S': 'created',\n",
    "        'emergencyId.S': 'emergency ID',\n",
    "        'senderId.S': 'sender ID',\n",
    "        'messageSentTimestamp.N': 'message sent timestamp'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3dd68-d326-4a80-9529-1dcf66e85de8",
   "metadata": {},
   "source": [
    "## 2.4. Changing Column Types\n",
    "\n",
    "We begin by examining the current data types of the columns using the `info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23b805-54ca-4a7b-914c-0b26c8379907",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f9496-0777-4ce7-9f9c-da15c1d57ef5",
   "metadata": {},
   "source": [
    "From the output of `chat.info()`, we can observe the structure of our DataFrame: it contains 6 columns and 16&nbsp;509 rows.\n",
    "\n",
    "This output also reveals that all columns are currently recognized as `object` type, which is a generic type for storing data in pandas.\n",
    "\n",
    "The `created` and `message sent timestamp` columns are currently formatted as objects but represent date-time information. The `created` column appears in ISO 8601 format (example value: `2024-03-23T21:44:36.788Z`), while `message sent timestamp` uses a Unix timestamp format (example value: `1711230277`). I will convert both to pandas datetime objects for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6b168-5b08-47c8-8d18-125211238d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting \"created\" from ISO 8601 format string to datetime.\n",
    "chat['created'] = pd.to_datetime(chat['created'])\n",
    "\n",
    "# Ensuring the \"created\" datetime column is timezone-unaware (naive).\n",
    "chat['created'] = chat['created'].dt.tz_localize(None)\n",
    "\n",
    "# Flooring the \"created\" datetime to the nearest second to remove any\n",
    "# smaller time units.\n",
    "chat['created'] = chat['created'].dt.floor('s')\n",
    "\n",
    "# Converting the \"message sent timestamp\" from Unix timestamp to a proper\n",
    "# datetime format.\n",
    "\n",
    "# First, ensuring the column is treated as an integer for accurate\n",
    "# datetime conversion.\n",
    "chat['message sent timestamp'] = chat['message sent timestamp'].astype(int)\n",
    "\n",
    "# Converting the integer timestamps in \"message sent timestamp\" to datetime\n",
    "# using Unix epoch time (seconds since 1970-01-01).\n",
    "chat['message sent timestamp'] = pd.to_datetime(\n",
    "    chat['message sent timestamp'], unit='s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c8278-4eea-4238-80f2-c32d6c6a47b0",
   "metadata": {},
   "source": [
    "From our earlier call to `chat.info()`, we identified two missing values in the `contents` column. We will remove these rows and then convert the `contents` column to a string type to ensure compatibility with text processing functions in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c4a81-57b8-4e15-8ce9-88d380fe32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with missing values in the \"contents\" column.\n",
    "chat = chat.dropna(subset=['contents'])\n",
    "\n",
    "# Converting \"contents\" to string type.\n",
    "chat['contents'] = chat['contents'].astype(str)\n",
    "\n",
    "# Displaying the DataFrame info again to show the changes.\n",
    "chat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309cbb0b-a709-48ae-9756-df2984cec82f",
   "metadata": {},
   "source": [
    "These conversions ensure that each column is optimized for the most appropriate and efficient data type.\n",
    "\n",
    "The columns containing ID information, such as `ID`, `emergency ID`, and `sender ID`, will not be used in the analyses planned for this project, and therefore will be left in their original format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d679cc",
   "metadata": {},
   "source": [
    "## 2.5. Removing System-generated Messages\n",
    "\n",
    "Now, let's sort our DataFrame by `created` and take a look at the first registered chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257d725-bc5a-4286-924b-1e752bba781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chat.sort_values(by='created')\n",
    "chat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697eb61-3f6a-4e41-a7fe-c5b329ac2fd4",
   "metadata": {},
   "source": [
    "The first messages seem to be system-generated, and I'm going to remove them.\n",
    "\n",
    "From now on, for the purpose of this project I'm going to focus on `contents` and `created` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce46b03-0d92-4a4d-abb8-4b9fe5ac9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retaining only the \"contents\" and \"created\" columns.\n",
    "chat = chat[['contents', 'created']]\n",
    "\n",
    "# Removing the system-generated messages.\n",
    "\n",
    "# Stripping leading and trailing whitespaces in the \"contents\" column first.\n",
    "chat['contents'] = chat['contents'].str.strip()\n",
    "\n",
    "# Filtering out rows containing the automated message.\n",
    "chat = chat[\n",
    "    ~chat['contents'].str.contains(\n",
    "        \"This emergency was submitted via the __\\\\*\\\\*Client Portal\\\\*\\\\*__\",\n",
    "        regex=True,\n",
    "        na=False\n",
    "    )\n",
    "]\n",
    "\n",
    "chat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8cf47",
   "metadata": {},
   "source": [
    "Now we see a different kind of system-generated messages in the first rows. Let's remove them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d4520-68da-496e-9d92-f07d25babdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows that start with \"## Emergency details from Client\".\n",
    "chat = chat[\n",
    "    ~chat['contents'].str.startswith(\n",
    "        \"## Emergency details from Client\", na=False\n",
    "    )\n",
    "]\n",
    "\n",
    "chat.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ae7e4",
   "metadata": {},
   "source": [
    "These messages are human-written.\n",
    "\n",
    "Let's see how many messages we have in our DataFrame now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5aae2",
   "metadata": {},
   "source": [
    "## 2.6. Exploratory Attempt to Filter Non-English Messages\n",
    "\n",
    "We could notice that not all messages were in English. To address this, I attempted to filter them out using the `langdetect` and `langid` libraries. \n",
    "\n",
    "`langdetect` identified 2&nbsp;742 messages as non-English, while `langid` detected 166 high-confidence messages in other languages. However, in both cases, most of the labeled messages were actually written in English. Consequently, I decided to discard the idea of filtering by language for our dataset.\n",
    "\n",
    "Here is the code used for these attempts:\n",
    "\n",
    "```\n",
    "pip install langdetect langid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1356ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "import langid\n",
    "\n",
    "# Function to detect the language using langdetect.\n",
    "def detect_language_langdetect(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Function to detect the language using langid and return both language code\n",
    "# and confidence.\n",
    "def detect_language_langid(text):\n",
    "    lang, confidence = langid.classify(text)\n",
    "    return lang, confidence\n",
    "\n",
    "# Applying langdetect.\n",
    "chat['language_langdetect'] = chat['contents'].apply(detect_language_langdetect)\n",
    "\n",
    "# Applying langid.\n",
    "chat[['language_langid', 'langid_confidence']] = chat['contents'].apply(\n",
    "    lambda x: pd.Series(detect_language_langid(x))\n",
    ")\n",
    "\n",
    "# Filtering messages detected as non-English by langdetect.\n",
    "non_english_langdetect = chat[chat['language_langdetect'] != 'en']\n",
    "\n",
    "# Filtering messages detected as non-English with high confidence by langid.\n",
    "high_confidence_threshold = 0.9\n",
    "high_confidence_non_english_langid = chat[\n",
    "    (chat['language_langid'] != 'en')\n",
    "    & (chat['langid_confidence'] >= high_confidence_threshold)\n",
    "]\n",
    "\n",
    "# Displaying the count of non-English messages detected.\n",
    "print(f\"langdetect detected {len(non_english_langdetect)} \"\n",
    "      \"non-English messages.\")\n",
    "print(f\"langid detected {len(high_confidence_non_english_langid)} \"\n",
    "      \"high-confidence non-English messages.\")\n",
    "\n",
    "# Displaying the top 10 detected non-English messages.\n",
    "top_10_non_english_langdetect = non_english_langdetect.head(10)\n",
    "display(top_10_non_english_langdetect[['contents', 'language_langdetect']])\n",
    "top_10_non_english_langid = high_confidence_non_english_langid.head(10)\n",
    "display(\n",
    "    top_10_non_english_langid[\n",
    "        ['contents', 'language_langid', 'langid_confidence']\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Cleaning up.\n",
    "chat.drop(\n",
    "    columns=['language_langdetect', 'language_langid', 'langid_confidence'],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65427a5",
   "metadata": {},
   "source": [
    "## 2.7. Text Preprocessing\n",
    "\n",
    "To prepare the chat messages for analysis, I perform several preprocessing steps on the text data. These steps include removing punctuation, converting all text to lowercase, and eliminating common stop words (like \"the\", \"is\", etc.) that are not useful for identifying key themes.\n",
    "\n",
    "Additionally, tokenization is part of this process. It breaks the text into individual words or phrases, allowing for more granular analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49183486-a971-464a-a80f-e5f940de1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing.\n",
    "def preprocess(text):\n",
    "    # Converting to lowercase.\n",
    "    text = text.lower()\n",
    "    # Tokenizing text.\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Removing non-alphabetic tokens.\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Applying preprocessing to the \"contents\" column.\n",
    "chat['processed_contents'] = chat['contents'].apply(preprocess)\n",
    "\n",
    "# Displaying the first 20 rows of the DataFrame to verify the preprocessing.\n",
    "chat.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98eb77-11e1-40c1-be50-46ac1531ffe5",
   "metadata": {},
   "source": [
    "# 3. Vectorization\n",
    "\n",
    "In this step, we transform the text data into a numerical format suitable for machine learning algorithms. We achieve this using TF-IDF (Term Frequency-Inverse Document Frequency), which helps in representing the importance of words in the context of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c663906-9272-40ce-b6f7-5fae31555e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the TF-IDF Vectorizer.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fitting the vectorizer to the processed text data and transforming the text\n",
    "# into numerical format.\n",
    "\n",
    "# \"X\" will be a sparse matrix where each row represents a text message\n",
    "# and each column represents a term.\n",
    "X = vectorizer.fit_transform(chat['processed_contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db18907-e82e-4464-8343-62d423303e5d",
   "metadata": {},
   "source": [
    "# 4. Initial Analysis\n",
    "## 4.1. Clustering\n",
    "\n",
    "In this section, I will use K-means clustering to group similar messages. This technique can help identify common themes or frequently discussed topics.\n",
    "\n",
    "The advantage of clustering is that it can group messages that are semantically similar even if they don't use the exact same words.\n",
    "\n",
    "I begin by retrieving the total number of CPU cores available on my PC to determine the computational resources available for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59604e7-0563-4713-a482-30509a3fd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc845281-04ec-4ee3-bffd-69a28aef3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the maximum number of CPU cores to be used by joblib to 8.\n",
    "# Joblib is a library for parallel computing in Python, used by scikit-learn\n",
    "# to speed up computations.\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '8'\n",
    "\n",
    "# Starting with the number of clusters equal to 5.\n",
    "k = 5\n",
    "\n",
    "# Initializing the KMeans clustering algorithm with \"k\" clusters.\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "\n",
    "# Fitting the KMeans model to the data and predicting cluster assignments.\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Attaching the predicted cluster labels back to the original DataFrame.\n",
    "chat['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa39b57-2cad-4f2f-a846-f54b76f54b31",
   "metadata": {},
   "source": [
    "Next, I will display the texts within each cluster to be able to identify common themes.\n",
    "\n",
    "To add perspective, I will calculate the size of each cluster and its percentage relative to the entire pool of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee22e3-dd44-4a38-a857-4296f61ea840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cluster sizes and percentages.\n",
    "cluster_counts = chat['cluster'].value_counts()\n",
    "total_counts = len(chat)\n",
    "cluster_percentages = ((cluster_counts / total_counts) * 100).round(2)\n",
    "\n",
    "# Sorting clusters by size.\n",
    "sorted_cluster_indices = cluster_counts.sort_values(ascending=False).index\n",
    "\n",
    "# Displaying clusters, sorted by cluster size.\n",
    "for i in sorted_cluster_indices:\n",
    "    # Sampling texts from each cluster.\n",
    "    sample = chat[chat['cluster'] == i]['contents'].sample(7).to_frame()\n",
    "    \n",
    "    # Getting size and percentage of the current cluster.\n",
    "    cluster_size = cluster_counts.loc[i]\n",
    "    cluster_percentage = cluster_percentages.loc[i]\n",
    "    \n",
    "    # Formatting the header with size and percentage, and displaying\n",
    "    # the sample texts.\n",
    "    display(HTML(f\"<h3>Cluster {i}: Size = {cluster_size}, \"\n",
    "                 f\"Percentage = {cluster_percentage}%</h3>\"))\n",
    "    display(HTML(sample.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ce26c",
   "metadata": {},
   "source": [
    "We can notice that a large cluster, comprising almost 4% of the data, consists of another type of automatic messages. We will use this discovery later to remove those messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fa14f-edc8-47dd-bdac-dee3e93caf0a",
   "metadata": {},
   "source": [
    "## 4.2. Topic Modeling with NMF\n",
    "\n",
    "Non-negative Matrix Factorization (NMF) is a topic modeling technique that can be used to discover the hidden thematic structure in large archives of text. Unlike K-means, NMF is a soft clustering method, meaning that each document can be associated with multiple topics, each with a certain weight.\n",
    "\n",
    "Let’s apply NMF to our dataset and explore its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5cc34f-1565-4136-baa5-86c0d3648489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using TF-IDF Vectorizer for NMF.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(chat['processed_contents'])\n",
    "\n",
    "# Applying NMF to the TF-IDF features.\n",
    "nmf_model = NMF(n_components=5, random_state=1, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Retrieving the feature names (words) from the TF-IDF vectorizer.\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Displaying topics and their key words.\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"Topic #{topic_idx}:\")\n",
    "    # Displaying top 10 words for each topic.\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205dd76a-f14d-4e9a-a7f2-b9aa8ee6c0e1",
   "metadata": {},
   "source": [
    "## 4.3. Evaluation of the First Results\n",
    "\n",
    "### Evaluation of Clustering\n",
    "\n",
    "- **Largest Cluster (82%)**: This cluster primarily consists of miscellaneous messages that do not fit into other clusters.\n",
    "\n",
    "- **Second Largest Cluster (9%)**: Predominantly centered around the words \"send/sent/sending,\" this cluster mostly involves discussions about friend requests or party invites, both essential for the community's rescue services.\n",
    "\n",
    "- **Third Largest Cluster (4%)**: This cluster reveals another type of pre-written message appearing in slightly different forms.\n",
    "\n",
    "- **Second Smallest Cluster (3%)**: Messages in this cluster frequently use the word \"ready\", typically asking if a person is prepared to receive an invite.\n",
    "\n",
    "- **Smallest Cluster (2%)**: This cluster is formed around the word \"sorry\", used in various contexts.\n",
    "\n",
    "### Evaluation of Topic Modeling\n",
    "\n",
    "- **Topic #0**: Reflects the theme of the pre-written message discovered during clustering: \"Alert received, please stand by, the Team Lead assigned\".\n",
    "\n",
    "- **Topic #1**: Centers on sending a friend request and a party invite, and asking the recipient to press the left bracket.\n",
    "\n",
    "- **Topic #2**: Related to Topic #1, with phrases like \"Let me know when you're ready to receive the invites\".\n",
    "\n",
    "- **Topic #3**: Appears to be a generic confirmation message: \"Copy, thanks, alright\".\n",
    "\n",
    "- **Topic #4**: Resembles a goodbye message: \"Thank you for choosing [Organization] services, stay safe\".\n",
    "\n",
    "### Summary and Next Steps\n",
    "\n",
    "While both approaches provide valuable insights, I found the clustering results easier to interpret. I will continue to work with clustering, adjusting the number of clusters to extract further insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debc2ce",
   "metadata": {},
   "source": [
    "# 5. Fine-Tuning Clustering\n",
    "\n",
    "## 5.1. Removing Identified Pre-written Messages\n",
    "\n",
    "From the analysis of the clusters, it became apparent that there is an existing pre-written type of message:\n",
    "\n",
    "> Thank you for choosing [Organization], your alert has been received.  A [Organization] team will be deployed to assist you shortly, please stand by to accept a friend request and a party invite from our Team Lead. (To accept the invitations, You should follow the Team Leader Instructions)  * *The Team Lead assigned, will inform you, when they are sending their invites.*  After you joined the party, please stand by to answer a few follow-up questions, so that we can provide a better service.\n",
    "\n",
    "I will remove the rows containing this message before re-running the clustering algorithm. This adjustment will help focus on more variable and unique interactions, potentially revealing deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62141efb-0728-41af-9b2e-698fa1b93ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a regular expression pattern that accounts for possible variations.\n",
    "# This regex will be flexible with spaces and punctuation.\n",
    "pattern = (\n",
    "    r\"thank you for choosing [Organization], your alert has been received\\..*\"\n",
    "    r\"please stand by to accept a friend request and a party invite \"\n",
    "    r\"from our team lead\\.\"\n",
    ")\n",
    "\n",
    "# Using the regex pattern to filter out rows.\n",
    "# The \"flags=re.I\" parameter makes the match case insensitive.\n",
    "chat = chat[\n",
    "    ~chat['contents'].str.contains(\n",
    "        pattern, case=False, na=False, regex=True, flags=re.I\n",
    "    )\n",
    "]\n",
    "\n",
    "# Checking how many messages remain in our DataFrame.\n",
    "len(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab0bda",
   "metadata": {},
   "source": [
    "## 5.2. Determining the Optimal Number of Clusters\n",
    "\n",
    "In this chapter, I will focus on finding the optimal number of clusters for our dataset. I will use two commonly applied methods to achieve this: the Elbow Method and the Silhouette Score Method.\n",
    "\n",
    "### Elbow Method\n",
    "The Elbow Method helps to determine the optimal number of clusters by plotting the sum of squared distances from each point to its assigned cluster center (within-cluster sum of squares) against the number of clusters. The point at which the rate of decrease sharply slows down, forming an \"elbow\", suggests an optimal cluster count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e93e65-4d13-4f20-9556-2360e11623c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty list to store the Within-Cluster Sum of Squares\n",
    "# (WCSS) values.\n",
    "wcss = []\n",
    "\n",
    "# Looping through a range of cluster numbers from 1 to 50.\n",
    "for i in range(1, 50):\n",
    "    # Initializing the KMeans clustering algorithm with \"i\" clusters.\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0\n",
    "    )\n",
    "    # Fitting the KMeans model to the data.\n",
    "    kmeans.fit(X)  # \"X\" is my vectorized data from earlier steps.\n",
    "    wcss.append(kmeans.inertia_)  # \"inertia_\" is the WCSS for the given model.\n",
    "\n",
    "# Plotting the results to visualize the Elbow Method.\n",
    "plt.plot(range(1, 50), wcss)\n",
    "plt.title(\"The Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e7d1b",
   "metadata": {},
   "source": [
    "### Silhouette Score Method\n",
    "\n",
    "The Silhouette Score Method evaluates the quality of clusters by measuring how similar each point is to its own cluster compared to other clusters. Scores range from -1 to 1, with higher scores indicating better-defined clusters. By plotting the Silhouette Score against different numbers of clusters, we can identify the cluster count that maximizes this score, indicating the optimal clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c2b17-1846-465b-a105-a85815f82e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty list to store the silhouette scores.\n",
    "silhouette_scores = []\n",
    "\n",
    "# Defining the range of cluster numbers to evaluate, starting from 2\n",
    "# because the silhouette score cannot be calculated for a single cluster.\n",
    "K_range = range(2, 50)\n",
    "\n",
    "# Looping through the range of cluster numbers.\n",
    "for k in K_range:\n",
    "    # Initializing the KMeans clustering algorithm with \"k\" clusters.\n",
    "    kmeans = KMeans(n_clusters=k, random_state=10)\n",
    "    # Fitting the KMeans model to the data and predicting cluster labels.\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    # Calculating the average silhouette score for the current number of clusters.\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    # Appending the silhouette score to the list.\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plotting the silhouette scores to visualize the results.\n",
    "plt.plot(K_range, silhouette_scores)\n",
    "plt.title(\"Silhouette Score Method\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48ea96",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "The Elbow Method did not show a clear \"elbow\", even with an extended range of clusters.\n",
    "\n",
    "Additionally, the highest Silhouette Score obtained was 0.1, which is generally considered low and indicates poorly separated clusters.\n",
    "\n",
    "These findings suggest that the data might not naturally separate into distinct groups very well, or there could be a high degree of overlap in the structure of the data points (chat messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede1500",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "After iterative testing, where I ran the clustering algorithm several times with different numbers of clusters, I found that setting the number of clusters to **14** is optimal. I focused only on clusters containing more than 50 messages. A smaller number of clusters does not reveal more nuanced, yet still valuable themes, while a larger number of clusters starts \"overfitting\", grouping messages by frequently used but very abstract themes, such as the verbs \"see\" and \"get\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61746132",
   "metadata": {},
   "source": [
    "# 6. Final Clustering Execution\n",
    "\n",
    "## 6.1. Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b394ed-5f8c-4740-9b23-3b65d8b89c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-vectorizing the \"processed_contents\" column after filtering out some rows\n",
    "# in the earlier steps.\n",
    "X = vectorizer.fit_transform(chat['processed_contents'])\n",
    "\n",
    "# The chosen optimal number of clusters.\n",
    "k = 14\n",
    "\n",
    "# Initializing the KMeans clustering algorithm with \"k\" clusters.\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "\n",
    "# Fitting the KMeans model to the data and predicting cluster assignments.\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Rewriting the existing \"cluster\" column with new clusters.\n",
    "chat['cluster'] = clusters\n",
    "\n",
    "# Calculating cluster sizes and percentages.\n",
    "cluster_counts = chat['cluster'].value_counts()\n",
    "total_counts = len(chat)\n",
    "cluster_percentages = ((cluster_counts / total_counts) * 100).round(2)\n",
    "\n",
    "# Sorting clusters by size.\n",
    "sorted_cluster_indices = cluster_counts.sort_values(ascending=False).index\n",
    "\n",
    "# Displaying clusters, sorted by cluster size.\n",
    "for i in sorted_cluster_indices:\n",
    "    # Getting size and percentage of the current cluster.\n",
    "    cluster_size = cluster_counts.loc[i]\n",
    "    cluster_percentage = cluster_percentages.loc[i]\n",
    "    \n",
    "    # Only displaying clusters with more than 50 messages.\n",
    "    if cluster_size > 50:\n",
    "        # Sampling texts from each cluster.\n",
    "        sample = chat[chat['cluster'] == i]['contents'].sample(7).to_frame()\n",
    "        \n",
    "        # Formatting the header with size and percentage, and displaying\n",
    "        # the sample texts.\n",
    "        display(HTML(f\"<h3>Cluster {i}: Size = {cluster_size}, \"\n",
    "                     f\"Percentage = {cluster_percentage}%</h3>\"))\n",
    "        display(HTML(sample.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233ce46",
   "metadata": {},
   "source": [
    "## 6.2. Cluster Analysis\n",
    "\n",
    "After filtering out system-generated and pre-written messages, the analysis revealed that a significant portion of the remaining chat messages - **66%** - couldn't be clustered in a meaningful way. However, several prominent themes emerged from the clustered messages:\n",
    "\n",
    "- **778 messages** contained \"Friend request sent\".\n",
    "- **667 messages** mentioned that \"Team is on their way now\".\n",
    "- **573 messages** included \"Sending party invite\".\n",
    "- **410 messages** stated \"Let me know when you are ready for the invites\".\n",
    "- **355 messages** said \"Thank you\", with additional **265 messages** saying \"Thanks\".\n",
    "- **279 messages** said \"Sorry\".\n",
    "- **207 messages** said \"Copy that\", with additional **78 messages** saying \"Roger that\".\n",
    "- **138 messages** mentioned \"beacon\".\n",
    "- **104 messages** said \"Accepted\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdfdf70",
   "metadata": {},
   "source": [
    "# 7. Project Conclusion and Recommendations\n",
    "\n",
    "The most frequently used and lengthy messages that are written manually are related to sending friend requests and party invites. These steps occur at the very beginning of the alert process and are mandatory. The analysis indicates that automating these messages would significantly reduce the manual effort required by the team.\n",
    "\n",
    "- Automating the message **\"Friend Request sent. Please press the Left Bracket key `[`\"** could streamline up to **6%** of the messages currently being written manually.\n",
    "\n",
    "- Automating **\"Sending party invite. Please be in first-person to accept\"** could streamline up to **4.5%**.\n",
    "\n",
    "- Automating **\"Let me know when you are ready to receive the invitations\"** could streamline up to **3.2%**.\n",
    "\n",
    "Together, automating these three messages could streamline up to **13.7%** of the manually written messages.\n",
    "\n",
    "Additionally, there are several short, frequently repeated messages that could also be considered for automation:\n",
    "\n",
    "- **\"Thank you\"** - 2.8%\n",
    "- **\"Sorry\"** - 2.2%\n",
    "- **\"Thanks\"** - 2.1%\n",
    "- **\"Copy that\"** - 1.6%\n",
    "- **\"Roger that\"** - 0.6%\n",
    "\n",
    "Assuming approximately 75% of these shorter messages are written by the team (and 25% by the client), automating them could streamline up to additional **7%** of the messages. Combined with the invitation-related messages, this could result in automating up to **20.7%** of the messages.\n",
    "\n",
    "Depending on the available options for automation, prioritizing the invitation-related messages would provide the most significant benefit. However, if feasible, automating the shorter messages could further enhance communication efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
